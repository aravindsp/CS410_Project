{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read actors File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import exists\n",
    "import glob\n",
    "actors = pd.read_csv('../files/actors.csv', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor Count before Filter: 28176\n",
      "28176\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nconst</th>\n",
       "      <th>primaryName</th>\n",
       "      <th>tconst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nm0000004</td>\n",
       "      <td>John Belushi</td>\n",
       "      <td>tt0077975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nm0000261</td>\n",
       "      <td>Karen Allen</td>\n",
       "      <td>tt0077975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nm0001371</td>\n",
       "      <td>Tom Hulce</td>\n",
       "      <td>tt0077975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nm0299122</td>\n",
       "      <td>Stephen Furst</td>\n",
       "      <td>tt0077975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nm0000004</td>\n",
       "      <td>John Belushi</td>\n",
       "      <td>tt0078723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      nconst    primaryName     tconst\n",
       "0  nm0000004   John Belushi  tt0077975\n",
       "1  nm0000261    Karen Allen  tt0077975\n",
       "2  nm0001371      Tom Hulce  tt0077975\n",
       "3  nm0299122  Stephen Furst  tt0077975\n",
       "4  nm0000004   John Belushi  tt0078723"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Actor Count before Filter:\",len(actors))\n",
    "#actors = actors[0:1000]\n",
    "actors = actors.reset_index(drop=True)\n",
    "print(len(actors))\n",
    "actors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Unique Titles for Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8905\n"
     ]
    }
   ],
   "source": [
    "uniqTitles = list(set(actors['tconst']))\n",
    "uniqTitles.sort()\n",
    "print(len(uniqTitles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Related Fuctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import re \n",
    "import urllib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a webdriver object and set options for headless browsing\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Chrome('../chromedriver',options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uses webdriver object to execute javascript code and get dynamically loaded webcontent\n",
    "def get_js_soup(url,driver):\n",
    "    driver.get(url)\n",
    "    res_html = driver.execute_script('return document.body.innerHTML')\n",
    "    soup = BeautifulSoup(res_html,'html.parser') #beautiful soup object to be used for parsing html content\n",
    "    return soup\n",
    "\n",
    "#tidies extracted text \n",
    "def process_mov(mov):\n",
    "    mov = mov.encode('ascii',errors='ignore').decode('utf-8')       #removes non-ascii characters\n",
    "    mov = re.sub('\\s+',' ',mov)       #repalces repeated whitespace characters with single space\n",
    "    return mov\n",
    "\n",
    "''' More tidying\n",
    "Sometimes the text extracted HTML webpage may contain javascript code and some style elements. \n",
    "This function removes script and style tags from HTML so that extracted text does not contain them.\n",
    "'''\n",
    "def remove_script(soup):\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    return soup\n",
    "\n",
    "\n",
    "#Checks if bio_url is a valid faculty homepage\n",
    "def is_valid_homepage(mov_url,dir_url):\n",
    "    if mov_url.endswith('.pdf'): #we're not parsing pdfs\n",
    "        return False\n",
    "    try:\n",
    "        #sometimes the homepage url points to the same page as the faculty profile page\n",
    "        #which should be treated differently from an actual homepage\n",
    "        ret_url = urllib.request.urlopen(mov_url).geturl() \n",
    "    except:\n",
    "        return False       #unable to access bio_url\n",
    "    urls = [re.sub('((https?://)|(www.))','',url) for url in [ret_url,dir_url]] #removes url scheme (https,http or www) \n",
    "    return not(urls[0]== urls[1])\n",
    "\n",
    "def scrape_movie_page(mov_url,driver):\n",
    "    soup = get_js_soup(mov_url,driver)\n",
    "    homepage_found = False\n",
    "    #profile_sec = soup.find('section',class_='main-content')\n",
    "    profile_sec = soup.find('div',class_='article listo')\n",
    "    if profile_sec == None:\n",
    "        mov = ''\n",
    "        print(\"Skipping mov : \",mov_url)\n",
    "    else:\n",
    "        mov = process_mov(profile_sec.get_text(separator=' '))\n",
    "    return mov_url,mov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_lst(lst,file_):\n",
    "    lst = lst.replace(' Plot Keywords Sort By: Relevance Alphabetical Showing all ',' ')\n",
    "    lst = lst.replace(' found this relevant Relevant? Yes No ',' ')\n",
    "    lst = lst.replace(' Is this relevant? Relevant? Yes No ',' ')\n",
    "    lst = lst.replace(' 1 of 1 ',' ').replace(' 0 of 1 ',' ')\n",
    "    lst = lst.replace(' 2 of 2 ',' ').replace(' 0 of 2 ',' ').replace(' 1 of 2 ',' ')\n",
    "    lst = lst.replace(' 3 of 3 ',' ').replace(' 0 of 3 ',' ').replace(' 1 of 3 ',' ').replace(' 2 of 3 ',' ')\n",
    "    lst = lst.replace(' Plot Showing all ','').replace(' items Jump to: Summaries (','')\n",
    "    lst = lst.replace('  ',' ').replace('   ',' ')\n",
    "\n",
    "    lst = lst.replace('Edit ','')\n",
    "\n",
    "\n",
    "    with open(file_,'w') as f:\n",
    "        f.write(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Movie Tags and store in \"movieFileTags\" dir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "8905\n",
      "8905 0\n"
     ]
    }
   ],
   "source": [
    "fileList = glob.glob(\"../movieFileTags/*.txt\")\n",
    "\n",
    "if len(fileList) <= 0:\n",
    "    last_scraped = 'a'\n",
    "else:\n",
    "    last_scraped = max(fileList).replace('../movieFileTags/','').replace('.txt','')\n",
    "\n",
    "print(last_scraped)\n",
    "print(len(uniqTitles))\n",
    "uniqTitles = sorted(i for i in uniqTitles if i > last_scraped)\n",
    "print(len(uniqTitles),len(fileList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Scraping movie url 1/8905 --------------------\n",
      "https://www.imdb.com/title/tt0035423/keywords\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'movieFileTags/tt0035423.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8a0a2cbe22ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmov\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmov_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mmov_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'movieFileTags/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mwrite_lst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmov\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmov_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-e0dbad2c1324>\u001b[0m in \u001b[0;36mwrite_lst\u001b[0;34m(lst, file_)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'movieFileTags/tt0035423.txt'"
     ]
    }
   ],
   "source": [
    "#Scrape homepages of all urls\n",
    "tot_urls = len(uniqTitles)\n",
    "for i,l in enumerate(uniqTitles):\n",
    "    link=\"https://www.imdb.com/title/\"+l+\"/keywords\"\n",
    "    print ('-'*20,'Scraping movie url {}/{}'.format(i+1,tot_urls),'-'*20)\n",
    "    print(link)\n",
    "    mov_urls, movs = [],[]\n",
    "    mov_url,mov = scrape_movie_page(link,driver)\n",
    "    if len(mov) < 10:\n",
    "        print(\"SKIPPING\")\n",
    "    else:\n",
    "        if mov.strip()!= '' and mov_url.strip()!='':\n",
    "            mov_file = '../movieFileTags/'+l+'.txt'\n",
    "            write_lst(mov,mov_file)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileList = glob.glob(\"../movieFileTags/*.txt\")\n",
    "activeMovs = []\n",
    "for i in fileList:\n",
    "    activeMovs.append(i.replace('../movieFileTags/','').replace('.txt',''))\n",
    "    \n",
    "df_act_movs = pd.DataFrame(activeMovs)\n",
    "df_act_movs.columns = ['tconst']\n",
    "df_act_movs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(actors))\n",
    "actors = actors.merge(df_act_movs,how='inner')\n",
    "print(len(actors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare actors list for scraped movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(actors))\n",
    "actors.to_csv('../files/actors.csv',header=True,index=False)\n",
    "actors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_act_movs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
